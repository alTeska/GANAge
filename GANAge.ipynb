{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/g12-36000.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-24ea5291a0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#loading models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mg12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_conv_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mstate_G12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/g12-36000.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mg12\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_G12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ateska/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/g12-36000.pkl'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.backends import cudnn\n",
    "from data_loader import get_loader\n",
    "from data_utils import FaceData\n",
    "from model import G12, G21, D1, D2\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cudnn.benchmark = True \n",
    "\n",
    "def merge_images(sources, targets, batch_size=9, k=10):\n",
    "    _, h, w = sources.shape\n",
    "    row = int(np.sqrt(batch_size))\n",
    "    merged = np.zeros([3, row*h, row*w*2])\n",
    "    for idx, (s, t) in enumerate(zip(sources, targets)):\n",
    "        i = idx // row\n",
    "        j = idx % row\n",
    "        merged[:, i*h:(i+1)*h, (j*2)*h:(j*2+1)*h] = s\n",
    "        merged[:, i*h:(i+1)*h, (j*2+1)*h:(j*2+2)*h] = t\n",
    "    return merged.transpose(1, 2, 0)\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "config1 = {\n",
    "'image_size' : 200, \n",
    "'g_conv_dim' : 64, \n",
    "'d_conv_dim' : 64, \n",
    "'num_classes' : 10, \n",
    "'train_iters' : 40000,\n",
    "'batch_size' : 9,\n",
    "'num_workers' : 2,\n",
    "'lr' : 0.0002,\n",
    "'beta1' : 0.5,\n",
    "'beta2' : 0.999,\n",
    "\n",
    "'mode'  :'train',\n",
    "'model_path' :'./models',\n",
    "'sample_path' :'./samples',\n",
    "'mnist_path' :'./mnist',\n",
    "'svhn_path' :'./svhn',\n",
    "'log_step' : 10,\n",
    "'sample_step' : 500,\n",
    "\n",
    "'use_labels': False, \n",
    "'use_reconst_loss' : True, \n",
    "}\n",
    "\n",
    "config = dotdict(config1)\n",
    "\n",
    "#loading data\n",
    "old = FaceData(image_paths_file='LAG/train/train.txt', young=False)\n",
    "young = FaceData(image_paths_file='LAG/train/train.txt')\n",
    "\n",
    "#loading models\n",
    "g12 = G12(conv_dim=config.g_conv_dim)\n",
    "state_G12 = torch.load('models/g12-36000.pkl')\n",
    "g12.load_state_dict(state_G12)\n",
    "\n",
    "d1 = D1(conv_dim=config.g_conv_dim)\n",
    "state_D1 = torch.load('models/d1-36000.pkl')\n",
    "d1.load_state_dict(state_D1)\n",
    "\n",
    "g21 = G21(conv_dim=config.g_conv_dim)\n",
    "state_G21 = torch.load('models/g21-36000.pkl')\n",
    "g21.load_state_dict(state_G21)\n",
    "\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "plt.ion()\n",
    "\n",
    "#plt.figure(figsize=(15,5))\n",
    "plt.figure()\n",
    "\n",
    "num_example_imgs = 3\n",
    "#for i, (img, img_label) in enumerate(young[-(num_example_imgs):]):\n",
    "for i, (img, img_label) in enumerate(young[-(num_example_imgs+100):]):\n",
    "\n",
    "    inputs = img.unsqueeze(0)\n",
    "    inputs = Variable(inputs)\n",
    "    \n",
    "    outputs = g12(inputs)\n",
    "    d1_out = d1(outputs)\n",
    "    #print(d1_out)\n",
    "    reverse = g21(outputs)    \n",
    "    \n",
    "    pred = outputs[0].data.cpu()\n",
    "    rev = reverse[0].data.cpu()\n",
    "    \n",
    "    img, pred, rev = img.numpy(), pred.numpy(), rev.numpy() \n",
    "\n",
    "    #pred = np.around(pred,decimals=2)\n",
    "    #pred = (pred - np.min(pred))*255/np.max(pred)\n",
    "    #rev = np.around(rev,decimals=2)\n",
    "    #rev = rev - np.min(rev)\n",
    "\n",
    "    #merged = merge_images(pred, rev)\n",
    "\n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.transpose(1,2,0))\n",
    "    \n",
    "    # pred\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 2)\n",
    "    plt.axis('off')\n",
    "    #plt.imshow(merged)\n",
    "    plt.imshow(pred.reshape(200,200,3))\n",
    "    \n",
    "    # rev\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 3)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(rev.reshape(200,200,3))\n",
    "    \n",
    "    plt.savefig('randomfig.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
